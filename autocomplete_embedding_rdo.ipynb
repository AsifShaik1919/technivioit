{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adopted from https://github.com/NVDLI/LDL. This code example is similar to autocomplete_rdo, but it works on words (encoded with an embedding layer) instead of characters and it does not do beam search. This version uses recurrent dropout and thus takes longer to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization code below contains a couple of additional imports compared to c11_e1_autocomplete and defines two new constants MAX_WORDS and EMBEDDING_WIDTH that define the max size of our vocabulary and the dimensionality of the word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 21:41:32.968603: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 21:41:32.976972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744252892.986609    2997 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744252892.989630    2997 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 21:41:33.000409: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = './frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "PREDICT_LENGTH = 3\n",
    "MAX_WORDS = 7500\n",
    "EMBEDDING_WIDTH = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet first reads the input file and splits the text into a list of individual words. The latter is done by using the imported function text_to_word_sequence(), which also removes punctuation and converts the text to lowercase. We then create input fragments and associated target words just as in the character-based example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read file.\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8-sig')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lower case and split into individual words.\n",
    "text = text_to_word_sequence(text)\n",
    "\n",
    "# Create training examples.\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the training examples into the correct format. Each input word needs to be encoded to a corresponding word index (an integer). This index will then be converted into an embedding by the Embedding layer. The target (output) word should still be one-hot encoded. To simplify how to interpret the output, we want the one-hot encoding to be done in such a way that bit N is hot when the network outputs the word corresponding to index N in the input encoding.\n",
    "\n",
    "We make use of the Keras Tokenizer class. When we construct our tokenizer, we provide an argument num_words = MAX_WORDS that caps the size of the vocabulary. The tokenizer object reserves index 0 to use as a special padding value and index 1 for unknown words. The remaining 9,998 indices (MAX_WORDS was set to 10,000) are used to represent words in the vocabulary.\n",
    "\n",
    "The padding value (index 0) can be used to make all training examples within the same batch have the same length. The Embedding layer can be instructed to ignore this value, so the network does not train on the padding values.\n",
    "\n",
    "Index 1 is reserved for UNKnown (UNK) words because we have declared UNK as an out-of-vocabulary (oov) token. When using the tokenizer to convert text to tokens, any word that is not in the vocabulary will be replaced by the word UNK. Similarly, if we try to convert an index that is not assigned to a word, the tokenizer will return UNK. If we do not set the oov_token parameter, it will simply ignore such words/indices.\n",
    "\n",
    "After instantiating our tokenizer, we call fit_on_texts() with our entire text corpus, which will result in the tokenizer assigning indices to words. We can then use the function texts_to_sequences to convert a text string into a list of indices, where unknown words will be assigned the index 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to indices.\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(text)\n",
    "fragments_indexed = tokenizer.texts_to_sequences(fragments)\n",
    "targets_indexed = tokenizer.texts_to_sequences(targets)\n",
    "\n",
    "# Convert to appropriate input and output formats.\n",
    "X = np.array(fragments_indexed, dtype=np.int64)\n",
    "y = np.zeros((len(targets_indexed), MAX_WORDS))\n",
    "for i, target_index in enumerate(targets_indexed):\n",
    "    y[i, target_index] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet creates a model with an Embedding layer followed by two long short-term memory (LSTM) layers, followed by one fully connected layer with ReLU activation, and finally a fully connected layer with softmax as output. When we declare the Embedding layer, we provide it with its input dimensions (vocabulary size) and output dimensions (embedding width) and tell it to mask inputs using index 0. This masking is not necessary for our programming example given that we created the training input such that all input examples have the same length, but we do it for good practice. We state input_length=None so that we can feed training examples of any length to the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744252895.409783    2997 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9209 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">750,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7500</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">967,500</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m256\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m750,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m256\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m117,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m7500\u001b[0m)            │       \u001b[38;5;34m967,500\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,982,844</span> (7.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,982,844\u001b[0m (7.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,982,844</span> (7.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,982,844\u001b[0m (7.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "98/98 - 70s - 716ms/step - loss: 7.0977 - val_loss: 7.9030\n",
      "Epoch 2/32\n",
      "98/98 - 67s - 680ms/step - loss: 6.4011 - val_loss: 8.1041\n",
      "Epoch 3/32\n",
      "98/98 - 67s - 686ms/step - loss: 6.2725 - val_loss: 8.2346\n",
      "Epoch 4/32\n",
      "98/98 - 68s - 692ms/step - loss: 6.1670 - val_loss: 8.2492\n",
      "Epoch 5/32\n",
      "98/98 - 67s - 688ms/step - loss: 6.0475 - val_loss: 8.4881\n",
      "Epoch 6/32\n",
      "98/98 - 67s - 682ms/step - loss: 5.9384 - val_loss: 8.4887\n",
      "Epoch 7/32\n",
      "98/98 - 67s - 683ms/step - loss: 5.8545 - val_loss: 8.6307\n",
      "Epoch 8/32\n",
      "98/98 - 66s - 677ms/step - loss: 5.7842 - val_loss: 8.7321\n",
      "Epoch 9/32\n",
      "98/98 - 66s - 677ms/step - loss: 5.7161 - val_loss: 8.7801\n",
      "Epoch 10/32\n",
      "98/98 - 70s - 718ms/step - loss: 5.6478 - val_loss: 8.9717\n",
      "Epoch 11/32\n",
      "98/98 - 66s - 676ms/step - loss: 5.5721 - val_loss: 8.7901\n",
      "Epoch 12/32\n",
      "98/98 - 66s - 669ms/step - loss: 5.4918 - val_loss: 8.9392\n",
      "Epoch 13/32\n",
      "98/98 - 67s - 681ms/step - loss: 5.4150 - val_loss: 9.0884\n",
      "Epoch 14/32\n",
      "98/98 - 68s - 698ms/step - loss: 5.3421 - val_loss: 9.1978\n",
      "Epoch 15/32\n",
      "98/98 - 67s - 688ms/step - loss: 5.2657 - val_loss: 9.3265\n",
      "Epoch 16/32\n",
      "98/98 - 67s - 688ms/step - loss: 5.1928 - val_loss: 9.3685\n",
      "Epoch 17/32\n",
      "98/98 - 67s - 683ms/step - loss: 5.1224 - val_loss: 9.5463\n",
      "Epoch 18/32\n",
      "98/98 - 67s - 684ms/step - loss: 5.0546 - val_loss: 9.7410\n",
      "Epoch 19/32\n",
      "98/98 - 67s - 680ms/step - loss: 4.9925 - val_loss: 9.7733\n",
      "Epoch 20/32\n",
      "98/98 - 67s - 684ms/step - loss: 4.9329 - val_loss: 10.0451\n",
      "Epoch 21/32\n",
      "98/98 - 66s - 677ms/step - loss: 4.8812 - val_loss: 10.0797\n",
      "Epoch 22/32\n",
      "98/98 - 67s - 681ms/step - loss: 4.8277 - val_loss: 10.2371\n",
      "Epoch 23/32\n",
      "98/98 - 67s - 681ms/step - loss: 4.7800 - val_loss: 10.4557\n",
      "Epoch 24/32\n",
      "98/98 - 67s - 684ms/step - loss: 4.7255 - val_loss: 10.7175\n",
      "Epoch 25/32\n",
      "98/98 - 65s - 666ms/step - loss: 4.6797 - val_loss: 10.7133\n",
      "Epoch 26/32\n",
      "98/98 - 67s - 682ms/step - loss: 4.6342 - val_loss: 10.8654\n",
      "Epoch 27/32\n",
      "98/98 - 67s - 680ms/step - loss: 4.5906 - val_loss: 11.0455\n",
      "Epoch 28/32\n",
      "98/98 - 66s - 673ms/step - loss: 4.5404 - val_loss: 11.1424\n",
      "Epoch 29/32\n",
      "98/98 - 67s - 679ms/step - loss: 4.4973 - val_loss: 11.3867\n",
      "Epoch 30/32\n",
      "98/98 - 66s - 674ms/step - loss: 4.4475 - val_loss: 11.5673\n",
      "Epoch 31/32\n",
      "98/98 - 67s - 689ms/step - loss: 4.4040 - val_loss: 11.7554\n",
      "Epoch 32/32\n",
      "98/98 - 67s - 687ms/step - loss: 4.3624 - val_loss: 11.7161\n"
     ]
    }
   ],
   "source": [
    "# Build and train model.\n",
    "training_model = Sequential()\n",
    "training_model.add(Input(shape=(None,), batch_size=BATCH_SIZE))\n",
    "training_model.add(Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS,\n",
    "    mask_zero=True))\n",
    "training_model.add(LSTM(128, return_sequences=True,\n",
    "                        dropout=0.2, recurrent_dropout=0.2))\n",
    "training_model.add(LSTM(128, dropout=0.2,\n",
    "                        recurrent_dropout=0.2))\n",
    "training_model.add(Dense(128, activation='relu'))\n",
    "training_model.add(Dense(MAX_WORDS, activation='softmax'))\n",
    "training_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam')\n",
    "training_model.summary()\n",
    "history = training_model.fit(X, y, validation_split=0.05,\n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             epochs=EPOCHS, verbose=2, \n",
    "                             shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we are ready to use it to do predictions. We do this a little bit differently than in the previous chapters. Instead of feeding a string of symbols as input to the model, we feed it only a single symbol at a time. This is an alternative implementation compared to the implementation in c11e1_autocomplete, where we repeatedly fed the model a growing sequence of characters (see the book for a more detailed description).\n",
    "\n",
    "The scheme used in this chapter has a subtle implication, which has to do with dependencies between multiple consecutive calls to model.predict(). We want the LSTM layers to retain their c and h states from one call to another so that the outputs of subsequent calls to predict() will depend on the prior calls to predict(). This can be done by giving the parameter stateful=True to the LSTM layers. A side effect of this is that we manually need to call reset_states() on the model before our first prediction.\n",
    "\n",
    "The code snippet below creates a model that is identical to the training model except that we declare the LSTM layers with stateful=True as well as specify a fixed batch size (required when declaring the LSTM layer as stateful) of size 1 using the batch_input_shape argument. We will transfer the weights from the trained model to this new model and use it for inference. This is done in the two last lines in the code snippet. There, we first read out the weights from the trained model and then initialize it into our inference model. For this to work, the models must have identical topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stateful model used for prediction.\n",
    "inference_model = Sequential()\n",
    "inference_model.add(Input(shape=(None,), batch_size=1))\n",
    "inference_model.add(Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS,\n",
    "    mask_zero=False))\n",
    "inference_model.add(LSTM(128, return_sequences=True,\n",
    "                         dropout=0.2, recurrent_dropout=0.2,\n",
    "                         stateful=True))\n",
    "inference_model.add(LSTM(128, dropout=0.2,\n",
    "                         recurrent_dropout=0.2, stateful=True))\n",
    "inference_model.add(Dense(128, activation='relu'))\n",
    "inference_model.add(Dense(MAX_WORDS, activation='softmax'))\n",
    "weights = training_model.get_weights()\n",
    "inference_model.set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet implements logic of presenting a word to the model and retrieving the word with the highest probability from the output. This word is then fed back as input to the model in the next timestep. The resulting autocompleted text sequence is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i saw the most creature \n"
     ]
    }
   ],
   "source": [
    "# Provide beginning of sentence and\n",
    "# predict next words in a greedy manner\n",
    "first_words = ['i', 'saw']\n",
    "first_words_indexed = tokenizer.texts_to_sequences(\n",
    "    first_words)\n",
    "inference_model.layers[1].reset_states()\n",
    "inference_model.layers[2].reset_states()\n",
    "predicted_string = ''\n",
    "# Feed initial words to the model.\n",
    "for i, word_index in enumerate(first_words_indexed):\n",
    "    x = np.zeros((1, 1), dtype=np.int64)\n",
    "    x[0][0] = word_index[0]\n",
    "    predicted_string += first_words[i]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "# Predict PREDICT_LENGTH words.\n",
    "for i in range(PREDICT_LENGTH):\n",
    "    new_word_index = np.argmax(y_predict)\n",
    "    word = tokenizer.sequences_to_texts(\n",
    "        [[new_word_index]])\n",
    "    x[0][0] = new_word_index\n",
    "    predicted_string += word[0]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "print(predicted_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the preceding code had to do with building and using a language model. The next code snippet adds some functionality to explore the learned embeddings. We first read out the word embeddings from the Embedding layer by calling get_weights() on layer 0, which represents the Embedding layer. We then declare a list of a number of arbitrary lookup words. This is followed by a loop that does one iteration per lookup word. The loop uses the Tokenizer to convert the lookup word to a word index, which is then used to retrieve the corresponding word embedding. The Tokenizer functions are generally assumed to work on lists. Therefore, although we work with a single word at a time, we need to provide it as a list of size 1, and then we need to retrieve element zero ([0]) from the output.\n",
    "\n",
    "Once we have retrieved the corresponding word embedding, we loop through all the other embeddings and calculate the Euclidean distance to the embedding for the lookup word using the NumPy function norm(). We add the distance and the corresponding word to the dictionary word_indices. Once we have calculated the distance to each word, we simply sort the distances and retrieve the five word indices that correspond to the word embeddings that are closest in vector space. We use the Tokenizer to convert these indices back to words and print them and their corresponding distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words close to: the\n",
      "the:  0.0\n",
      "a:  3.8265364\n",
      "“the:  4.5998383\n",
      "an:  4.7560496\n",
      "his:  5.2432733\n",
      "\n",
      "words close to: saw\n",
      "saw:  0.0\n",
      "see:  0.47161064\n",
      "seeing:  0.48724994\n",
      "incomplete:  0.49550736\n",
      "ban—as:  0.50558364\n",
      "\n",
      "words close to: see\n",
      "see:  0.0\n",
      "saw:  0.47161064\n",
      "distrust:  0.51923317\n",
      "which:  0.52073133\n",
      "sharing:  0.5220382\n",
      "\n",
      "words close to: of\n",
      "of:  0.0\n",
      "in:  0.3527701\n",
      "with:  0.37208176\n",
      "by:  0.39471143\n",
      "all:  0.40955588\n",
      "\n",
      "words close to: and\n",
      "and:  0.0\n",
      "not:  0.37156314\n",
      "deserved:  0.4141283\n",
      "caroline:  0.42683053\n",
      "populous:  0.42983887\n",
      "\n",
      "words close to: monster\n",
      "monster:  0.0\n",
      "“fear:  0.4875126\n",
      "wouldst:  0.49422055\n",
      "nonproprietary:  0.49492013\n",
      "ashes:  0.4983188\n",
      "\n",
      "words close to: frankenstein\n",
      "frankenstein:  0.0\n",
      "redistribution:  0.44618255\n",
      "efforts:  0.4595991\n",
      "refund:  0.46755794\n",
      "holder:  0.46883\n",
      "\n",
      "words close to: read\n",
      "read:  0.0\n",
      "fierceness:  0.4762353\n",
      "charge:  0.5105593\n",
      "heart:  0.5150057\n",
      "travel:  0.5172955\n",
      "\n",
      "words close to: eat\n",
      "eat:  0.0\n",
      "forbear:  0.50898314\n",
      "magnificent:  0.5213033\n",
      "unhallowed:  0.52408516\n",
      "private:  0.52441317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore embedding similarities.\n",
    "embeddings = training_model.layers[0].get_weights()[0]\n",
    "lookup_words = ['the', 'saw', 'see', 'of', 'and',\n",
    "                'monster', 'frankenstein', 'read', 'eat']\n",
    "for lookup_word in lookup_words:\n",
    "    lookup_word_indexed = tokenizer.texts_to_sequences(\n",
    "        [lookup_word])\n",
    "    print('words close to:', lookup_word)\n",
    "    lookup_embedding = embeddings[lookup_word_indexed[0]]\n",
    "    word_indices = {}\n",
    "    # Calculate distances.\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        distance = np.linalg.norm(\n",
    "            embedding - lookup_embedding)\n",
    "        word_indices[distance] = i\n",
    "    # Print sorted by distance.\n",
    "    for distance in sorted(word_indices.keys())[:5]:\n",
    "        word_index = word_indices[distance]\n",
    "        word = tokenizer.sequences_to_texts([[word_index]])[0]\n",
    "        print(word + ': ', distance)\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
